{
  "title": "Jan 1: Continual Lifelong Learning with Neural Networks: A Review",
  "cells": [
    {
      "type": "markdown",
      "data": "## Parent note\n[Jan 1: Online/lifelong deep learning](quiver-note-url/A9B6B858-DAA0-498C-9F42-7E39E40EB314)\n"
    },
    {
      "type": "markdown",
      "data": "# Overall Thoughts:\n- Could reasoning be used to help mitigate catastrophic forgetting? There could be less catastrophic forgetting when correcting its mistake for a misclassified sample... but what about a new class? If predicting saliency is relatively independent to class (but can this be if we're saying saliency is predictive of the class?) then for a given class, we narrow down the size of the architecture that needs to be updated.\n- Saliency could help with few-shot learning.\n- Can the NN detect when it's losing something? And, assuming that this approach is different from just regularizing, can humans provide \"reasoning\" to the network that resolves this conflict? \n\nSide thoughts: \n- Should make something that tells you which categories are likely catastrophically forgotten after having trained a network on a new dataset.\n- A curriculum builder based on one axis of difficulty.\n"
    },
    {
      "type": "markdown",
      "data": "## 1 Introduction\n- Training a model with new information interferes with previously learned knowledge\n- Retraining would potentially counter this problem, but this isn't efficienty\n- Biological analogy to this challenge is the \"stability-plasticity dilemma\"\n\n### 1.1 Biology\n- Two types of plasticity for stable lifelong process:\n  - Hebbian (positive feedback instability)\n  - Compensatory homeostatic plasticity (for stability)\n- CLS theory (Complementary Learning Systems): extracting statistical structure of perceived events (generalization) while retaining *episodic* memories. Humans can both generalize across experiences and remember specific memorie\n- Neocortex -- slow learning rate in it is for learning \"generalities\". However catastrophic forgetting can still occur.\n\n### 1.2 Machine Learning\nModels incorporate the following characteristics:\n- Regulate \"intrinsic levels of synaptic plasticity\" to protect consolidated knowledge\n- allocate additional neural resources to learn new information\n- Complementary learning systems (CLS) for \"memory consolidation and memory replay\"\nThe above are focused on supervised learning\n"
    },
    {
      "type": "markdown",
      "data": "## 2 Hebbian Plasticity and Stability\nHebbs: When one neuron drives activity of another neuron, the connection between them is strengthened. With synaptic strenght $w$, $w$ updates proportional to both the presynaptic and postsynaptic activity, weighted by $\\eta$.\n$$\\Delta w = x \\cdot y \\cdot \\eta$$\nHebbian systems get stability when you put constraints to the upper limits on synaptic weights $w$.\n\nHomeostatic plasticity is a modulatory effect or feedback control signal that regulates unstable dynamics of Hebbian plasticity\n$$ \\Delta w = m \\cdot x \\cdot y \\cdot \\eta$$\n$m$ is modulatory signal \n(For more info: [Zenke et al. 2017](https://ganguli-gang.stanford.edu/pdf/16.HebbianLearningHomeostaticPlasticity.pdf))\n(Interesting work: read [Zenke et al., 2017](https://arxiv.org/pdf/1703.04200.pdf))"
    },
    {
      "type": "markdown",
      "data": "## 3 Complementary Learning Systems\n- Brain must generalize across experiences while consolidating episodic memories.\n- Brain has different learning rates in differen areas\n- Hyppocampus allows for rapid learning, plays back over time to the neocortical system for long-term retention\n  - Hyppocampus encodes sparse representations of events to minimize interference\n  - Neocortex has slow learning rate and builds \"overlapping representations of learned knowledge\"\n  - Both learn using \"Hebbian\" and \"Error-driven\" mechanisms (O'Reilly and Rudy, 2001)\n  \n- When neurogenesis happens, the new cell at first has a very high learning rate, and decreases to make new memory more stable\n- Hippocampus --> prefrontal cortex, and happens usually during REM sleep\n\n- Hippocampus supports additional forms of generalizations from recurrent interaction of episodic memories\n- If new information is consistent with existing knowledge, then its integration into the neocortex is faster\n"
    },
    {
      "type": "markdown",
      "data": "## 4 Lifelong learning in NN's\nCatastrophic forgetting studied in networks using backprop and Hopfield networks\n- can be completely overcome by allocating additinoal neurla resources whenver they are required\n  - Scalability issues in large neural architectures\n  - Non-trivial to predefine a siufficient amount of neural resources that will prevent catastrophic forgetting\n\n3 aspects of avoiding catastrophic forgetting in connectionist models:\n- allocating additional neural resources for new knowledge\n- using \"non-overlapping\" (?) representations if resources are fixed\n- interleaving old knowledge as the new information is represented\n\nCategories of tackling catastrophic forgetting:\n- retraining neural network with regularization\n- selectively train the network and expand it if necessary\n- Memory replay to relearn\n\n### 4.1 Regularization approaches\n#### 4.1.1 Very brute force formulation\nLearning without forgetting -- uses knowledge distillation (i.e. transfering knowledge from large regularized networks to smaller ones)\n\n$$\\theta_s^*, \\theta_o^*, \\theta_n^* \\leftarrow argmin_{\\hat{\\theta}_s, \\hat{\\theta}_o, \\hat{\\theta}_n} \\biggr ( \\lambda_o \\mathcal L_{old}(Y_o, \\hat{Y}_o) + \\mathcal L_{new}(Y_n, \\hat{Y}_n) + \\mathcal R(\\hat{\\theta}_s, \\hat{\\theta}_o, \\hat{\\theta}_n) \\biggr )$$\nwhere $\\theta_s$ is the common weights, $\\theta_o$ is the old weights for old tasks and $\\theta_n$ are the weights for the new task. $R$ is regularization based on the weights. $Y_o$ and $\\hat{Y_o}$ represents the ground truth and predictions on the old tasks using $\\theta_s$, $\\theta_n$ and $\\theta_o$, respectively. $Y_n$ and $\\hat{Y_n}$ are the corresponding ground truth and predictions for the new tasks using all weights.\nDrawbacks of method:\n- dependent on relevance of the tasks\n- Training time fo rone task slinearly increases with the number of learned tasks\n- Needs reservoir of data for each learned task\n\n#### 4.1.2 Regularization of hidden layer\nAnother method suggests to use regularization on the last hidden layer (Jung et al. 2016):\n[Arxiv](https://arxiv.org/abs/1607.00122)\n[Paper](https://arxiv.org/pdf/1607.00122.pdf)\n[Running TODO’s](quiver-note-url/A9542605-1DA6-43CA-A640-62BEC2551F87)\nDrawbacks of method:\n- computationally expensive since old task's paremeters need to be recomputed for each novel data sample\n\nOther approaches freeze the weights for the old tasks (Razavian et al., 2014) or reduce learning rate to prevent huge changes.\n\n#### 4.1.3 Elastic weight consolidation (EWC)\nQuadratic penalty on difference betweenn paremeters for the old and new takss. Meant to slow down the learning for weights conding for previously learned knowledge. \n\nRelevance of parameter $\\theta$ with respect to training data $D$ is given by $p(\\theta | \\mathcal D)$.\nGiven independent tasks with datasets $D_A$ and $D_B$, by baye's rule:\n$$\\log p(\\theta | \\mathcal D) = \\log p(\\mathcal D_{B} | \\theta) + \\log p(\\theta | \\mathcal D_{A}) - \\log p(\\mathcal D_{B})$$\nEWC approximates $\\log p(\\theta | D_{A})$ as a guassian distribution with mean given by \\theta^*_A and daigonal precision (variance?) given by Fisher information matrix $F$.\n\nLoss function defined by:\n$\\mathcal L(\\theta) = \\mathcal L_B(\\theta) + \\sum_{i} (\\lambda/2) F_i(\\theta_i - \\theta^*_{A, i})^2$\n\nThis is shown to outperform other methods in \"permutation tasks\" but not capable of learning new categories.\n\n#### 4.1.4 Individual synapse estimating importance\n(read more [Zenke et al., 2017](https://arxiv.org/pdf/1703.04200.pdf))\n[Running TODO’s](quiver-note-url/A9542605-1DA6-43CA-A640-62BEC2551F87)\n\nIndividual synapses get to estimate their importance in older tasks, and force updates to happen to neurons that are less important.\n\nLoss $\\mathcal L^*_n$ is given by:\n$$ \\mathcal L^*_n = \\mathcal L_n + c \\sum_k \\Omega_k^n(\\theta_k^* - \\theta_k)^2$$\n- $c$ is weighting parameter\n- $\\Omega_k^n $: per-parameter regulation strength\n\nSynaptic relevance computed in an online fashion across learning trajectory."
    },
    {
      "type": "markdown",
      "data": "### 4.2 Dynamic Architectures:\nRetraining with an increased number of neurons or networks layers. Rusu et al. (2016) suggested to block any changes trained on previous knowledge and expand the architectyre by allocating novel sub-networks. This is called \"progressive networks\". Given $N$ existing tasks, wanna learn $T_{N+1}$. $\\theta^n$ is left unchanged but $\\theta^{n+1}$ is learned for $T_{N+1}$. This is unsustainable though as th ecomplexity of the architecture grows with the number of learned tasks.\n\nZhou et al. (2012) proposed incremental training of a \"denoising autoencoder\" that adsds additional neurons for samples with high loss and subsequently merges these neurons with existing ones to prevent redundancy.\n- Add new features to minimize the residual of the objective function\n- Merging similar features to obtain compact feature representation. \nThis outperformans non-incremental denoising autoencoders in MNIST and CIFAR-10\n\nCortes et al (2016) balances model complexity with empirical risk minimization. Performs well on binary classification tasks from CIFAR-10 dataset.\n\nXiao et al. (2014) proposed training algorithm with a network that grows in capacity in a hierarchical fashion. Classes are grouped according to their similarity and self-organized into multiple levels, models inheriting features to speed up learning. Only topmost layers can grow and the vanilla back-propagation is inefficient. (_Why is that? Could saliency solve it?_) \n(Read more: [Paper](https://www.microsoft.com/en-us/research/wp-content/uploads/2014/11/Error-Driven-Incremental-Learning-in-Deep-Convolutional-Neural-Network-for-Large-Scale-Image-Classification.pdf), [Original webpage](https://www.microsoft.com/en-us/research/publication/error-driven-incremental-learning-in-deep-convolutional-neural-network-for-large-scale-image-classification/))\n[Running TODO’s](quiver-note-url/A9542605-1DA6-43CA-A640-62BEC2551F87)\n\nFernando et al. (2017) proposed a method where genetic algorithms used to find \"optimal path\" through the network to be reused for learning new tasks. Other weights are frozen to combat catastrophic forgetting. PathNet requires independent output layers for each new task, which prevents it from learning new classes incrementally (_Why is that?_)\n\nDraelos et al. (2017) Used autoencoder, and added new neural unit sto the autoencoder to facilitate additions of new MNIST digits. Uses \"intrinsic replay\" -- a generative model used for pseudo-rehearsal -- to preserve the weights required to retrina older information. (Read more: [Arxiv](https://arxiv.org/abs/1612.03770), [Paper](https://arxiv.org/pdf/1612.03770.pdf) [Running TODO’s](quiver-note-url/A9542605-1DA6-43CA-A640-62BEC2551F87))\n\nYoon et al. (2018) took concept to the sueprvisedslearning paradigm (_important for us!_) and created \"dynamically expanding network\", increasing number of trainable paremters ot learn new tasks. DEN is trained by performing \"selective retraining\" which expands the network using \"group sparse regularization\" to decide how many neurons to add to each layer. (Read more: [Arxiv](https://arxiv.org/abs/1708.01547), [Paper](https://arxiv.org/pdf/1708.01547.pdf), [Running TODO’s](quiver-note-url/A9542605-1DA6-43CA-A640-62BEC2551F87))\n\nPart and Lemon (2016, 2017) combines pre-trained CNN with \"self-organizing incremental neurla network\" to allow the netowrk to grow. Issue is scalability, since the size of the netowrk grows with the number of classes that have been learned (_but isn't this the problem with other approaches too in this category?_). Another problem is that the pre-trained network will be conditioned on the dataset used to the train it, and might not match the new-task distribution. Rebuffi et al. (2017) stores example data points that are used along with the new data (Similar to interweaving old and new data) to \"adapt the weights of the feature extractor\". Higher memory footprint this way (_and also seems kinda stupid ..._), but can \"prevent\" catastrophic forgetting.\n\nParisi (_this guy seems to do a lot in this area_)et al. (2017) came up with something to do learning on the human action sequences but the review paper does a bad job explaining it (Read more: [Science Direct](https://www.sciencedirect.com/science/article/pii/S0893608017302034)[Paper](https://ac.els-cdn.com/S0893608017302034/1-s2.0-S0893608017302034-main.pdf?_tid=e6e6cfd2-4a12-4c09-8387-ba587da903de&acdnat=1546434276_b515c4d85563def0113f0786e535af15))\n"
    },
    {
      "type": "markdown",
      "data": "### 4.3 Dual-memory learning systems\nCLS theory driven -- interplay of the hippocampus and the neocortex, between generalization and memorization. (_Question - Could the neural net tell us whether the answer that they give us is from the generalization or from episodic memory? Could be an interesting idea._) \n\nEarly computational example of CLS was by Hinton and Plaut (1987). Each synaptic connection has two weights -- plastic weight with slow change rate and fast changing weight for temporary knowledge. \n\nFrench (1997) developed a \"pseudo-recurrent dual-memory framework\", one for early processing an danother for long-term storage. Uses pseudo-rehearsals to transfer memories between memory centers. Training samples in pseudo rehearsals are drawn from a probabilistic model and not kept in mmeory.\n\nTheres is no empirical evidence to show that these approaches can scale up to a large number of tasks. \n\nSoltoggio (2015): Consolidating new information on the bases of a \"cause-effect hypothesis testing\". The differenc ebetween short- and long-term plasticity is not related to the duration of the memory but the \"confidence of consistency of cause-effect relationships\". This is called hypothesis testing plasticity (HTP). \n\nGepperth and Karaoguz (2016) proposed two approaches to incremental learning\n- a modified self-organizing map (SOM)\n- SOM extended with shor-term memory\n  - STM Has limited capacity, so doesn't really overcome catastrophic forgetting\n  \nShin et al. 2017 proposed dual-model architecture consisting of deep generative model and task solver. Training data from perviously learned tasks can be \"sampled\" from generating pseudo-data, and playing this back to the task solver. This way, working memory is dramatically decreased. \n\nLuders et al. 2016 proposed an \"evolvable\" Neural Turing Machine (ENTM) that allows agents to store long-term memories by progressively allocating additional memory components. The optimal structure for the network is found by evolving the network's topology and weights. ENTM will cause the agent to continually expand its memory over time. This can lead to the slowing of the learning process significantly.\n\nKemker and Kanan (2018) proposed FearNet, a trio-model architecture composing of\n- a network for the hippocampus\n- a generative networksfor the PFC\n- a network to emulate the basolateral amygdala for choosing which of the above systems to listen to.\nInformation from the hippcampus is loaded to the PFC during sleep phases. Essentially, during sleep phases, the PFC GAN is trained to output stuff that the hippocampus has recently seen. (\"To do this, FearNet’s PFC model is a generative neural network that creates pseudosamples that are then intermixed with recently observed examples stored in its hippocampal network.\") \n(Read more: [arxiv](https://arxiv.org/abs/1711.10563), [paper](https://arxiv.org/pdf/1711.10563.pdf), [Running TODO’s](quiver-note-url/A9542605-1DA6-43CA-A640-62BEC2551F87))\n\nKamra et al. (2018) presented a similar dual-memory farameowrk that uses a variational autoencoder as a generative model. Generates a short-term memory modual for each new taks. However, before consolidation, predictions are made using some oracle. (_the description of this in the review paper is pretty bad., refer to original paper_)\n(Read more: [arxiv](https://arxiv.org/abs/1710.10368), [paper](https://arxiv.org/pdf/1710.10368.pdf), [Running TODO’s](quiver-note-url/A9542605-1DA6-43CA-A640-62BEC2551F87))\n\nParisi et al. (2018) proposed dual-memory \"self-organizing architecture\" for lifelong learning of spatio-temporal representation from videos. But the description is pretty shit. Apparently does really well though on benchmark datasets. (Read more: [paper](https://www2.informatik.uni-hamburg.de/wtm/publications/2017/PBKWYLLW17/08329784.pdf)[Running TODO’s](quiver-note-url/A9542605-1DA6-43CA-A640-62BEC2551F87))\n"
    },
    {
      "type": "markdown",
      "data": "## 5 Benchmark datasets\nKemker et al. suggested a more exhaustive guideline for comparing methods, using 3 benchmark experiments:\n- data permutation\n  - training a model with a dataset and a version that is permuted. Expected that the model prevents catastrophic forgetting during the subseqwuent learning of randomly permuted data samples.\n- incremental class learning\n  - Ability of a model to remember information about older classes while incrementally learning one class at a time\n- multimodal learning\n  - the same model is sequentally trained with datasets of different modalities (what the fuck??)\n\nLopez-paz and Ronzalo (2017) introduced:\n- backwards transfer: how much influence previously learned tasks have on the current task\n- forward transfer: how much influence current task has on future tasks.\n(Read more: [Paper](https://arxiv.org/pdf/1706.08840.pdf), [Running TODO’s](quiver-note-url/A9542605-1DA6-43CA-A640-62BEC2551F87))\n\n\nOn the Caltech-UCSD Birds-200 dataset (200 different bird species) and AudioSet dataset (youtube 10-sec audios), found that:\n- for data permutation: 1) Pathnet, 2) EWC. Suggests that regularization works best when the new task have similar feature distributions\n- for multi-modal, EWC beats Pathnet, because EWC does a good job separating non-redundant data.\n- for incremental learning, best were obtained with a combination of rehearsal and dual-memory systems (GeppNet+STM). \n\n__There's more work to be done__: \"Perhaps not surprisingly, the evaluated models performed significantly worse when using datasets of higher complexity such as CUB-200 than when tested on the MNIST. This suggests that while there is a large number of approaches capable of alleviating catastrophic forgetting in highly controlled experimental conditions, lifelong learning has not been addressed for more complex scenarios.\"\n"
    },
    {
      "type": "markdown",
      "data": "## 6 Curriculum learning\nDevelopmental models acquire an increasingly complex set of skills based on their sensorimotor experiences. \"developmental strategies\" is a very complex practice. Non-trivial to select developmental stages. \"It remains unclear how to systematically define developmental stages on the basis of the interaction between innate structure, embodiment, and (active) inference\"\n\nElman (1993) shows that having a curriculum of progressively harder tasks helps the agent learn faster. The effectiveness of curriculum learning is highly sensitive to modality of progression through the tasks. Furthermore, it's very hard to assume that task can be ordered dby a single axis of difficulty. Graves et al (2017) frames this as a stochastic policy over tasks that maximizes  leraning progress (i.e. teacher tries to maximize performance of sstudent.) Curriculumn learning can be seen as a speicial case of transfer learning where prior tasks is used to guide the learning process of more sophisticated ones.\n(Read more: [paper](http://proceedings.mlr.press/v70/graves17a/graves17a.pdf), [Running TODO’s](quiver-note-url/A9542605-1DA6-43CA-A640-62BEC2551F87))\n"
    },
    {
      "type": "markdown",
      "data": "## 7 Transfer learning \nForwards transfer vs. backwards transfer\ntransfer is positive if learning about $T_a$ helpes with learning $T_b$, and negative otherwise. (__can an agent actively think about what part it already knows is transferrable? and can we decouple a single skill from the rest?__) The actual mechanism that does knowledge transfer in the brain is poorly understood, but people think that \"the transfer of abstract knowledge may be achieved through the use of conceptual representations that econde relational information invariant to idnividuals, objects or scene elements) (Doumas et al. 2008)\n\nZero/one-shot learning aim at performing well on novel tasks but _do not_ prevent catastrophic forgetting. \n\nLopez-Paz and Ranzato (2017) proposed Gradient Episodic Memory (GEM) that alleviates catastrophic forgetting and performs positive transfer to previously learned tasks.\n"
    },
    {
      "type": "markdown",
      "data": "## Intrinsic Motivation\nAlgorithms collect data and acquire skills incrementally through the online self-generation of a learning curriculum. "
    },
    {
      "type": "markdown",
      "data": "## Multi-modal learning\n\"Abstract representations obtained from a network encoding the source modality can be used to fine-tune the network in the target modality, thereby relaxing the imbalance of the available data in the target modality.\"\n- Interesting implications\n(Read More:[Arxiv](https://arxiv.org/abs/1412.3121))\n"
    }
  ]
}