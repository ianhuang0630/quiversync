{
  "title": "Dec 30: {C} Pytorch tutorial",
  "cells": [
    {
      "type": "markdown",
      "data": "## Basic functions"
    },
    {
      "type": "code",
      "language": "python",
      "data": "from __future__ import print_function\nimport torch\n\nx = torch.empty(5,3) # uninitialized matrix that temporarily stores values\nx = torch.rand(5,3) # 5x3 of random numbers between 0 and 1\nx = torch.tensor([5.5, 3]) # vector of floats\n\nx = x.new_ones(5,3, dtype=torch.double) # creates array of 1's\nx = torch.randn_like(x, dtype=torch.float) # 5x3 matrix of values from normal dist\n\nx.size() # gives the shape as a torch tensor"
    },
    {
      "type": "code",
      "language": "python",
      "data": "# x and y are tensors\nz = x + y\nz = torch.add(x,y)\n\nz = torch.empty(5,3)\ntorch.add(x, y, out=z)\n\n# adding in place \ny.add_(x)\n\n# more operations are found here: \n# https://pytorch.org/docs/torch"
    },
    {
      "type": "code",
      "language": "python",
      "data": "x[:, 1] # same slicing syntax as numpy\n\nx = torch.randn(4,4)\ny = x.view(16) # reshaping in pytorch\nz = x.view(-1, 8) # reshaping with inferred dimension\n"
    },
    {
      "type": "markdown",
      "data": "## Numpy compatibility"
    },
    {
      "type": "code",
      "language": "python",
      "data": "a = torch.ones(5)\nb = a.numpy() # converts to numpy\n\nb = np.ones(5)\na = torch.from_numpy(b) # converts to torch\n"
    },
    {
      "type": "markdown",
      "data": "## GPU"
    },
    {
      "type": "code",
      "language": "python",
      "data": "if torch.cuda.is_available(): # tells you if gpu available\n    device = torch.device('cuda')\n    y = torch.ones_like(x, device=device) # creates a tensor on GPU\n    x = x.to(device) # alternatively \".to('cuda')\"\n    z = x + y\n    print(z.to('cpu', torch.double)) # moving to cpu\n    "
    },
    {
      "type": "markdown",
      "data": "## Back propagation"
    },
    {
      "type": "code",
      "language": "python",
      "data": "x = torch.ones(2, 2, requires_grad=True)\ny = x+2\ny.grad_fn # gives you the gradient function\n# every operation based on a tensor with requires_grad = True has requires_grad= True\n\n\na = torch.randn(2,2)\na = ((a * 3) / (a-1))\n# a doesn't have requires_grad on\na.requires_grad_(True) # requires_grad_ sets it in-place\nb = (a*a).sum()\n\n\n\ny.backward() # prepares the gradient \\partial y / \\partial something\nprint(x.grad) # prints the gradient  \\partial y / \\partial x\n\nprint(x.requires_grad) # true\nwith torch.no_grad(): # disabling grad is useful for inference, saves memory\n    print((x**2).requires_grad) # false\n"
    },
    {
      "type": "markdown",
      "data": "## Neural net"
    },
    {
      "type": "code",
      "language": "python",
      "data": "import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Net(nn.Module):\n    def __init__(self):\n        super(Net, self).__init__()\n        self.conv1 = nn.Conv2d(1,6,5)\n        self.conv2 = nn.Conv2d(6,16, 5)\n        \n        self.fc1 = nn.Linear(16*5*5, 120)\n        self.fc2 = nn.Linear(120, 84)\n        self.fc3 = nn.Linear(84, 10)\n        \n    def forward(self, x):\n        # Max pooling over a (2, 2) window\n        x = F.max_pool2d(F.relu(self.conv1(x)), (2, 2))\n        # If the size is a square you can only specify a single number\n        x = F.max_pool2d(F.relu(self.conv2(x)), 2)\n        x = x.view(-1, self.num_flat_features(x))\n        x = F.relu(self.fc1(x))\n        x = F.relu(self.fc2(x))\n        x = self.fc3(x)\n        return x\n        \n    def num_flat_features(self, x):\n        size = x.size()[1:]  # all dimensions except the batch dimension\n        num_features = 1\n        for s in size:\n            num_features *= s\n        return num_features\nnet = Net()\nnet.parameters() # returns all the learnable weights\n\n                    #nSamples x nChannels x Height x Width.\ninput = torch.randn(1,1,32,32)\nout = net(input)\nprint(out)\nnet.zero_grad()\nout.backward(torch.randn(1,10))\n\n\n"
    },
    {
      "type": "code",
      "language": "python",
      "data": "output = net(input)\ntarget = torch.randn(10)\ntarget = target.view(1,-1)\ncriterion = nn.MSELoss()\n\nloss = criterion(output, target)\nprint(loss)\n\nnet.zero_grad()\nloss.background()\n\n# implementation of backprop\nlearning_rate = 0.01\nfor f in net.parameters():\n    f.data.sub_(f.grad.data * learning_rate) # in-place subtraction \n    "
    },
    {
      "type": "code",
      "language": "python",
      "data": "import torch.optim as optim\n\noptimizer = optim.SGD(net.parameters(), lr=0.01)\n\noptimizer.zero_grad() # this time you're not using net.zero_grad()\noutput = net(input)\nloss = criterion(output, target)\nloss.backwards() \noptimizers.step() #update the weights, given to opitmizer through net.parameters()\n"
    },
    {
      "type": "markdown",
      "data": "## Training a classifier"
    },
    {
      "type": "code",
      "language": "python",
      "data": "import torch.op;tim as optim\n# instantiate model\nnet = Net()\n# define a loss type \ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.SGD(net.paremeters(), lr=0.001, momentum=0.9)\n\nfor epoch in range(num_epochs):\n    running_loss = 0.0\n    for i, data in enumerate(trainloader, 0):\n        inputs, labels = data\n        optimizer.zero_grad()\n        outputs = net(inputs)\n        loss = criterion(outputs, labels)\n        loss.backward()\n        optimizer.step()\n        \n        running_loss += loss.item()\n        # ... printing out intermediary statuses\n\nprint ('Finished Training')"
    },
    {
      "type": "code",
      "language": "python",
      "data": "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n\nnet = Net()\nnet.to(device)\n# rest of the code in the previous block, with every tensor (inputs ..etc) moved to gpu in the same fashion."
    },
    {
      "type": "markdown",
      "data": "[For training on multiple GPU's at the same time](https://pytorch.org/tutorials/beginner/blitz/data_parallel_tutorial.html) "
    },
    {
      "type": "markdown",
      "data": "## Writing the dataloader\nMainly composed of writing a dataset class with a `__getitem__()` method, and then using that in `dataloader = Dataloader(dataset, ...other options)`. "
    },
    {
      "type": "code",
      "language": "python",
      "data": ""
    }
  ]
}