{
  "title": "Jan 1: Working with RedNet",
  "cells": [
    {
      "type": "markdown",
      "data": "## Rednet for Semantic Segmentation for Salicon\n\nSince Salicon's data is semantic segmentation based, one can compare the resnet architecture performance for saliency vs. no saliency. We'd have to remove the depth dimensions.\n"
    },
    {
      "type": "markdown",
      "data": "## Model architecture"
    },
    {
      "type": "code",
      "language": "python",
      "data": "class RedNet(nn.Module):\n    def __init__(self, num_classes=37, pretrained=False):\n\n        super(RedNet, self).__init__()\n        block = Bottleneck\n        transblock = TransBasicBlock\n        layers = [3, 4, 6, 3]\n        # original resnet\n        self.inplanes = 64\n        self.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3,\n                               bias=False)\n        self.bn1 = nn.BatchNorm2d(64)\n        self.relu = nn.ReLU(inplace=True)\n        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n        self.layer1 = self._make_layer(block, 64, layers[0])\n        self.layer2 = self._make_layer(block, 128, layers[1], stride=2)\n        self.layer3 = self._make_layer(block, 256, layers[2], stride=2)\n        self.layer4 = self._make_layer(block, 512, layers[3], stride=2)\n\n        # resnet for depth channel\n        self.inplanes = 64\n        self.conv1_d = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3,\n                                 bias=False)\n        self.bn1_d = nn.BatchNorm2d(64)\n        self.layer1_d = self._make_layer(block, 64, layers[0])\n        self.layer2_d = self._make_layer(block, 128, layers[1], stride=2)\n        self.layer3_d = self._make_layer(block, 256, layers[2], stride=2)\n        self.layer4_d = self._make_layer(block, 512, layers[3], stride=2)\n\n        self.inplanes = 512\n        self.deconv1 = self._make_transpose(transblock, 256, 6, stride=2)\n        self.deconv2 = self._make_transpose(transblock, 128, 4, stride=2)\n        self.deconv3 = self._make_transpose(transblock, 64, 3, stride=2)\n        self.deconv4 = self._make_transpose(transblock, 64, 3, stride=2)\n\n        self.agant0 = self._make_agant_layer(64, 64)\n        self.agant1 = self._make_agant_layer(64 * 4, 64)\n        self.agant2 = self._make_agant_layer(128 * 4, 128)\n        self.agant3 = self._make_agant_layer(256 * 4, 256)\n        self.agant4 = self._make_agant_layer(512 * 4, 512)\n\n        # final block\n        self.inplanes = 64\n        self.final_conv = self._make_transpose(transblock, 64, 3)\n\n        self.final_deconv = nn.ConvTranspose2d(self.inplanes, num_classes, kernel_size=2,\n                                               stride=2, padding=0, bias=True)\n\n        self.out5_conv = nn.Conv2d(256, num_classes, kernel_size=1, stride=1, bias=True)\n        self.out4_conv = nn.Conv2d(128, num_classes, kernel_size=1, stride=1, bias=True)\n        self.out3_conv = nn.Conv2d(64, num_classes, kernel_size=1, stride=1, bias=True)\n        self.out2_conv = nn.Conv2d(64, num_classes, kernel_size=1, stride=1, bias=True)\n\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n                m.weight.data.normal_(0, math.sqrt(2. / n))\n            elif isinstance(m, nn.BatchNorm2d):\n                m.weight.data.fill_(1)\n                m.bias.data.zero_()\n\n        if pretrained:\n            self._load_resnet_pretrained()\n"
    },
    {
      "type": "markdown",
      "data": "## Other methods\n"
    },
    {
      "type": "code",
      "language": "python",
      "data": "    def _make_layer(self, block, planes, blocks, stride=1):\n        downsample = None\n        if stride != 1 or self.inplanes != planes * block.expansion:\n            downsample = nn.Sequential(\n                nn.Conv2d(self.inplanes, planes * block.expansion,\n                          kernel_size=1, stride=stride, bias=False),\n                nn.BatchNorm2d(planes * block.expansion),\n            )\n\n        layers = []\n\n        layers.append(block(self.inplanes, planes, stride, downsample))\n        self.inplanes = planes * block.expansion\n        for i in range(1, blocks):\n            layers.append(block(self.inplanes, planes))\n\n        return nn.Sequential(*layers)\n        \n    def _make_transpose(self, block, planes, blocks, stride=1):\n\n        upsample = None\n        if stride != 1:\n            upsample = nn.Sequential(\n                nn.ConvTranspose2d(self.inplanes, planes,\n                                   kernel_size=2, stride=stride,\n                                   padding=0, bias=False),\n                nn.BatchNorm2d(planes),\n            )\n        elif self.inplanes != planes:\n            upsample = nn.Sequential(\n                nn.Conv2d(self.inplanes, planes,\n                          kernel_size=1, stride=stride, bias=False),\n                nn.BatchNorm2d(planes),\n            )\n\n        layers = []\n\n        for i in range(1, blocks):\n            layers.append(block(self.inplanes, self.inplanes))\n\n        layers.append(block(self.inplanes, planes, stride, upsample))\n        self.inplanes = planes\n\n        return nn.Sequential(*layers)\n\n    def _make_agant_layer(self, inplanes, planes):\n\n        layers = nn.Sequential(\n            nn.Conv2d(inplanes, planes, kernel_size=1,\n                      stride=1, padding=0, bias=False),\n            nn.BatchNorm2d(planes),\n            nn.ReLU(inplace=True)\n        )\n        return layers\n\n    def _load_resnet_pretrained(self):\n        pretrain_dict = model_zoo.load_url(utils.model_urls['resnet50'])\n        model_dict = {}\n        state_dict = self.state_dict()\n        for k, v in pretrain_dict.items():\n            if k in state_dict:\n                if k.startswith('conv1'):  # the first conv_op\n                    model_dict[k] = v\n                    model_dict[k.replace('conv1', 'conv1_d')] = torch.mean(v, 1).data. \\\n                        view_as(state_dict[k.replace('conv1', 'conv1_d')])\n\n                elif k.startswith('bn1'):\n                    model_dict[k] = v\n                    model_dict[k.replace('bn1', 'bn1_d')] = v\n                elif k.startswith('layer'):\n                    model_dict[k] = v\n                    model_dict[k[:6] + '_d' + k[6:]] = v\n        state_dict.update(model_dict)\n        self.load_state_dict(state_dict)\n\n    def forward_downsample(self, rgb, depth):\n\n        x = self.conv1(rgb)\n        x = self.bn1(x)\n        x = self.relu(x)\n        depth = self.conv1_d(depth)\n        depth = self.bn1_d(depth)\n        depth = self.relu(depth)\n\n        fuse0 = x + depth\n\n        x = self.maxpool(fuse0)\n        depth = self.maxpool(depth)\n\n        # block 1\n        x = self.layer1(x)\n        depth = self.layer1_d(depth)\n        fuse1 = x + depth\n        # block 2\n        x = self.layer2(fuse1)\n        depth = self.layer2_d(depth)\n        fuse2 = x + depth\n        # block 3\n        x = self.layer3(fuse2)\n        depth = self.layer3_d(depth)\n        fuse3 = x + depth\n        # block 4\n        x = self.layer4(fuse3)\n        depth = self.layer4_d(depth)\n        fuse4 = x + depth\n\n        return fuse0, fuse1, fuse2, fuse3, fuse4\n\n    def forward_upsample(self, fuse0, fuse1, fuse2, fuse3, fuse4):\n\n        agant4 = self.agant4(fuse4)\n        # upsample 1\n        x = self.deconv1(agant4)\n        if self.training:\n            out5 = self.out5_conv(x)\n        x = x + self.agant3(fuse3)\n        # upsample 2\n        x = self.deconv2(x)\n        if self.training:\n            out4 = self.out4_conv(x)\n        x = x + self.agant2(fuse2)\n        # upsample 3\n        x = self.deconv3(x)\n        if self.training:\n            out3 = self.out3_conv(x)\n        x = x + self.agant1(fuse1)\n        # upsample 4\n        x = self.deconv4(x)\n        if self.training:\n            out2 = self.out2_conv(x)\n        x = x + self.agant0(fuse0)\n        # final\n        x = self.final_conv(x)\n        out = self.final_deconv(x)\n\n        if self.training:\n            return out, out2, out3, out4, out5\n\n        return out\n\n    def forward(self, rgb, depth, phase_checkpoint=False):\n\n        if phase_checkpoint:\n            depth.requires_grad_()\n            fuses = checkpoint(self.forward_downsample, rgb, depth)\n            out = checkpoint(self.forward_upsample, *fuses)\n        else:\n            fuses = self.forward_downsample(rgb, depth)\n            out = self.forward_upsample(*fuses)\n\n        return out\n"
    },
    {
      "type": "markdown",
      "data": "## The Mysterious Bottleneck"
    },
    {
      "type": "code",
      "language": "python",
      "data": "class Bottleneck(nn.Module):\n    expansion = 4\n\n    def __init__(self, inplanes, planes, stride=1, downsample=None):\n        super(Bottleneck, self).__init__()\n        self.conv1 = nn.Conv2d(inplanes, planes, kernel_size=1, bias=False)\n        self.bn1 = nn.BatchNorm2d(planes)\n        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride,\n                               padding=1, bias=False)\n        self.bn2 = nn.BatchNorm2d(planes)\n        self.conv3 = nn.Conv2d(planes, planes * 4, kernel_size=1, bias=False)\n        self.bn3 = nn.BatchNorm2d(planes * 4)\n        self.relu = nn.ReLU(inplace=True)\n        self.downsample = downsample\n        self.stride = stride\n\n    def forward(self, x):\n        residual = x\n\n        out = self.conv1(x)\n        out = self.bn1(out)\n        out = self.relu(out)\n\n        out = self.conv2(out)\n        out = self.bn2(out)\n        out = self.relu(out)\n\n        out = self.conv3(out)\n        out = self.bn3(out)\n\n        if self.downsample is not None:\n            residual = self.downsample(x)\n\n        out += residual\n        out = self.relu(out)\n\n        return out"
    },
    {
      "type": "markdown",
      "data": "## TransBasicsBlock (whatever that is...)"
    },
    {
      "type": "code",
      "language": "python",
      "data": "class TransBasicBlock(nn.Module):\n    expansion = 1\n\n    def __init__(self, inplanes, planes, stride=1, upsample=None, **kwargs):\n        super(TransBasicBlock, self).__init__()\n        self.conv1 = conv3x3(inplanes, inplanes)\n        self.bn1 = nn.BatchNorm2d(inplanes)\n        self.relu = nn.ReLU(inplace=True)\n        if upsample is not None and stride != 1:\n            self.conv2 = nn.ConvTranspose2d(inplanes, planes,\n                                            kernel_size=3, stride=stride, padding=1,\n                                            output_padding=1, bias=False)\n        else:\n            self.conv2 = conv3x3(inplanes, planes, stride)\n        self.bn2 = nn.BatchNorm2d(planes)\n        self.upsample = upsample\n        self.stride = stride\n\n    def forward(self, x):\n        residual = x\n\n        out = self.conv1(x)\n        out = self.bn1(out)\n        out = self.relu(out)\n\n        out = self.conv2(out)\n        out = self.bn2(out)\n\n        if self.upsample is not None:\n            residual = self.upsample(x)\n\n        out += residual\n        out = self.relu(out)\n\n        return out"
    }
  ]
}