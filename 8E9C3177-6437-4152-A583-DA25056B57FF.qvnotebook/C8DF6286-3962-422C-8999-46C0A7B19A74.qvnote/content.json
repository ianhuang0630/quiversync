{
  "title": "Sept 9: Scene embeddings, based on proposal",
  "cells": [
    {
      "type": "text",
      "data": "<div style=\"word-wrap: break-word; -webkit-nbsp-mode: space; -webkit-line-break: after-white-space; white-space: normal;\"><div><span style=\"font-weight: bold;\">Context:</span></div><div>The deep TAMER framework is used to map a visual input embedding to a approximate reward <img src='quiver-image-url/F71B22B8EE3F74C77B216487C432A8AB'>. The environment in which deep TAMER was applied was radically more simplistic than that of this proposal. As such, the visual embedding technique (autoencoder) that worked well for Atari Bowling may not work so well for navigation under different simulated environments.</div><div><br /></div><div>The embeddings should contain some semantic significance. I believe that a key ability that robots should is recognizing the similarity between different setups of rooms. Effectively, this space would be used to contain abstractions of rooms that it’s seen before. These embeddings could allow for more generalizability (in reference to key issue #6 on the proposal) in cases not in the training set.</div><div><br /></div><div>One question I can’t answer right now: whether a similar embedding would be learned by the neural net. I guess they technically could be, but it’s hard to make sure of that. Providing structured a priori knowledge should make it strictly better than not doing so.</div><div><br /></div><div><span style=\"font-weight: bold;\">Goal: </span></div><div>Expedite the process of learning by embedding similarity between different scenes. Similarity should be high if the same items are located at around the same place in a frame. This should speed up training and also necessitate less human intervention.</div><div><br /></div><div>Possible definitions of the embedding function <en-medi<a href='quiver-file-url/'></a>div>Given a pair of destination (position of a pixel) and the current viewpoint, map to an embedding.</div><div><en-media hash=\"1934c4e0a6344e6e73f95e36d42806<a href='quiver-file-url/'></a> map to an embedding.</div><div><en-media hash=\"307629083de9e5fbc29bded3021e743a\" type=\"image/png\" /><br /></div><di<a href='quiver-file-url/'></a>ave to be done end-to-end with navigation tasks. This would take more time to implement (many moving parts) and we may not find if it works soon enough. As such, this writeup will focus on the second type of mapping.</div><div><br /></div><div><span style=\"font-weight: bold;\">How to train:</span></div><div><br /></div><div><b>One approach — defining our own loss function.</b></div><div>To speed up the training process, I believe these embeddings can be pretrained on the RGB-D data. However, doing so would basically prohibit access to <en-media hash=\"749437e3a8418d21f2569da79b92891e\" type=\"image/png\" />, since it depends on <br /></div><div><br /></div><d<a href='quiver-file-url/'></a>rchy):</div><ol><li><div>obstacles have similar location (3d location, since RGB-D data)</div></li><li><div>obstacle types are similar (this would be computer generated)</div></li><li><div>pixel level differences. (RGB)</div></li></ol><div><br /></div><div>Cost function:</div><div>A combination of the  of the above 3 points. </div><div><en-media hash=\"13eec341ad8cdc51c5daa4b94528216d\" type=\"image/png\" /><br /></div><div>where <en-media hash=\"cc364355a40232035680f84e6b3df0cd\" type=\"image/png\" /<a href='quiver-file-url/'></a>png\" /> would be the set of all center of masses of every obs<a href='quiver-file-url/'></a>8031ea908eb7955bccf2ed757ea2201\" type=\"image<a href='quiver-file-url/'></a>ash=\"17c30da26e6e54b34528b6c3c8eb4a1f\" type=\"image/png\" /> is the set of sets of pixels belonging to every class in <en-media hash=\"78031ea908eb<a href='quiver-file-url/'></a>similarity, CatSim evaluates category similarity and PixSim evaluates pixel-level simila<a href='quiver-file-url/'></a>ion would very likely be undifferentiable.</div><div><br /></div><div><span style=\"font-weight: <a href='quiver-file-url/'></a><li><div>Run of the mill deep neural network model, input is the image, and after layers and layers of convnets, creates two embeddings and outputs </div></li><ul><ul><li><div><en-media hash=\"f758781d16b4985f444577618264d951\" type=\"image/png\" /></div></li></ul></ul><li><div>Transfer learning — include parts of the 3D semantic mapping (<a href=\"https://arxiv.org/pdf/1609.05130.pdf\">https://arxiv.org/pdf/1609.05130.pdf</a>) network here, and add further transformations above here. Eventually, the embeddings would be </div></li></ul><div><br /></div><div><span <a href='quiver-file-url/'></a>nd to end. </b>can test end to end, and if the robot does better with similarity-embedded features than with auto-encoded features, we can conclude that it contributes to the robot’s performance.</div></li><li><div><b>Quantitative.</b> For two embeddings of two different scenes, evaluate how well their ‘difference’ corresponds to the true <i>L </i>value.</div></li><li><div><b>Qualitative. </b>ask model to find similar arrangements in a given set of images, and evaluate qualitatively how well it can find similar settings.</div></li></ol><div><br /></div><div><br /></div><div><br /></div><div><br /></div><div><br /></div></div>"
    }
  ]
}