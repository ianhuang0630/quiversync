{
  "title": "Oct 25: Human-Computer",
  "cells": [
    {
      "type": "text",
      "data": "<div style=\"word-wrap: break-word; -webkit-nbsp-mode: space; -webkit-line-break: after-white-space; white-space: normal;\"><div><span style=\"font-size: 36px;\"><span style=\"font-size: 36px; font-weight: bold;\">Methods</span></span></div><div>VQG Visual question generation</div><div><a href=\"https://arxiv.org/pdf/1712.01238.pdf\">https://arxiv.org/pdf/1712.01238.pdf</a></div><ul><li><div>relatively new paper about generating questions </div></li><li><div>paper maximizes expected accuracy for questions posed on image</div></li><li><div>we want to add an additional channel and change the objective to maximal certainty in the correct classes (maximizing the margin)</div></li></ul><div><br /></div><div><a href=\"https://arxiv.org/pdf/1612.04936.pdf\">https://arxiv.org/pdf/1612.04936.pdf</a></div><ul><li><div>Learn through dialogue and asking quesitons</div></li></ul><div><br /></div><div><a href=\"https://arxiv.org/pdf/1805.07674.pdf\">https://arxiv.org/pdf/1805.07674.pdf</a></div><ul><li><div>Chanxi zhang’s paper on metric embeddings, by Prof Eugene’s suggestion</div></li></ul><div><br /></div><div><span style=\"font-size: 36px; font-weight: bold;\">Fresh ideas</span></div><div><span style=\"font-weight: bold;\">Dictating properties (NEW): (see Nov 4 note)</span></div><div><span style=\"text-decoration: underline;\">Basic ideas</span></div><ul><li><div>When a robot sees something that it’s never seen before, instead of just providing it a new label for a class, explain <span style=\"font-style: italic;\">why</span> it belongs to that class.</div></li><li><div>Natural language can be used to pass apriori human knowledge to robot. This way, higher level descriptions of features are provided to the robot, and the robot will be able to better generalize</div></li><li><div>Specifically, it should be able to</div></li><ul><li><div>take in a NL sentence, and parse it into a graph of properties related to the object</div></li><li><div>pass the current scene through the network and extract some sort of embeddings</div></li><li><div>With respect to embeddings that it has developed for other classes and properties, it would determine a new embedding for this new class, and then update the weights to both better do class prediction, as well as the target embedding.</div></li><li><div>The mapping going from properties -&gt; embeddings will probably require some data-base lookup.</div></li></ul></ul><div><img src='quiver-image-url/D054D060FF679265BC078B63A105C48A.JPG' title='Attachment'><br /></div><div><br /></div><div><span style=\"font-weight: bold;\">Dictating attention (NEW):</span></div><div>[Relevant ] <a href=\"https://arxiv.org/pdf/1804.01720.pdf\">https://arxiv.org/pdf/1804.01720.pdf</a></div><ul><li><div>visual localization “Finding beans in burgers”</div></li></ul><div>[highly relevant] <a href=\"https://arxiv.org/pdf/1709.05307.pdf\">https://arxiv.org/pdf/1709.05307.pdf</a></div><ul><li><div>implemented saliency-enhanced classification for training. How well does it work for new classes?</div></li></ul><div><span style=\"text-decoration: underline;\">Basic ideas</span></div><ul><li><div>When the robot sees something that it’s never seen before, you provide a key area within the image for which the robot should pay attention to, as well as the class of the object</div></li><li><div>The network then needs to update its weights not only to best predict the class, but also to have the heat map focus most on where you previously pointed.</div></li><li><div>The benefit of this is that attention can be quickly given to the computer without having to train the network on many many images of the same object in order to get attention and correct classification.</div></li><li><div>The update would be done with a “reverse” back propagation. Would need to learn more about attention and saliency maps</div></li></ul><div><br /></div><div><br /></div><div><br /></div><div><span style=\"font-size: 36px; font-weight: bold;\">Datasets</span></div><div><a href=\"http://sceneparsing.csail.mit.edu/\">http://sceneparsing.csail.mit.edu/</a></div><ul><li><div>pretty good dataset, has both outdoor and indoor images. May have to sort out which is which individually</div></li></ul><div><a href=\"https://www2.eecs.berkeley.edu/Research/Projects/CS/vision/bsds/BSDS300/html/dataset/images.html\">https://www2.eecs.berkeley.edu/Research/Projects/CS/vision/bsds/BSDS300/html/dataset/images.html</a></div><div><br /></div><div><a href=\"https://cs.stanford.edu/people/jcjohns/clevr/\">https://cs.stanford.edu/people/jcjohns/clevr/</a></div><ul><li><div>CLEVR’s mostly used for VQA</div></li></ul><div><br /></div><div><a href=\"https://blog.playment.io/self-driving-car-datasets-semantic-segmentation/\">https://blog.playment.io/self-driving-car-datasets-semantic-segmentation/</a></div><ul><li><div>blog about some more datasets</div></li></ul><div><br /></div><div><span style=\"font-size: 36px;\"><span style=\"font-size: 36px; font-weight: bold;\">Others</span></span></div><div>Interesting blog about semantic segmentation with CNN’s over the years</div><div><a href=\"https://meetshah1995.github.io/semantic-segmentation/deep-learning/pytorch/visdom/2017/06/01/semantic-segmentation-over-the-years.html\">https://meetshah1995.github.io/semantic-segmentation/deep-learning/pytorch/visdom/2017/06/01/semantic-segmentation-over-the-years.html</a></div><div><br /></div><div><a href=\"https://www.pygaze.org/2018/05/saliency-mapping-taylor-swift/\">https://www.pygaze.org/2018/05/saliency-mapping-taylor-swift/</a></div><div><br /></div></div>"
    }
  ]
}