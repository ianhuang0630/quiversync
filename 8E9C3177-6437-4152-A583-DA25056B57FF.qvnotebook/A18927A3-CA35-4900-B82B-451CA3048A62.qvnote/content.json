{
  "title": "Aug 8: Action embedding: Action segmentation",
  "cells": [
    {
      "type": "text",
      "data": "<div style=\"word-wrap: break-word; -webkit-nbsp-mode: space; -webkit-line-break: after-white-space; white-space: normal;\"><div>CNN <a href=\"https://arxiv.org/abs/1611.05267\">https://arxiv.org/abs/1611.05267</a></div><div>Weakly supervised <a href=\"https://arxiv.org/abs/1803.10699\">https://arxiv.org/abs/1803.10699</a></div><div><a href=\"https://arxiv.org/abs/1801.09571\">https://arxiv.org/abs/1801.09571</a></div><div><a href=\"https://arxiv.org/abs/1803.05790\">https://arxiv.org/abs/1803.05790</a></div><div><br /></div><div><a href=\"http://users.cecs.anu.edu.au/~qshi/pub/ActRecog_IJCV08.pdf\">http://users.cecs.anu.edu.au/~qshi/pub/ActRecog_IJCV08.pdf</a></div><div><br /></div><div><br /></div><div><a href=\"https://openreview.net/forum?id=r1nzLmWAb\">https://openreview.net/forum?id=r1nzLmWAb</a></div><div><br /></div><div><a href=\"http://openaccess.thecvf.com/content_cvpr_2017/papers/Lea_Temporal_Convolutional_Networks_CVPR_2017_paper.pdf\">http://openaccess.thecvf.com/content_cvpr_2017/papers/Lea_Temporal_Convolutional_Networks_CVPR_2017_paper.pdf</a></div><div><br /></div><div><a href=\"https://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Lu_Human_Action_Segmentation_2015_CVPR_paper.pdf\">https://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Lu_Human_Action_Segmentation_2015_CVPR_paper.pdf</a></div><div><br /></div><div>Using entropy</div><div><a href=\"https://ieeexplore.ieee.org/ielx7/6287639/8274985/08244278.pdf?tp=&amp;arnumber=8244278&amp;isnumber=8274985\">https://ieeexplore.ieee.org/ielx7/6287639/8274985/08244278.pdf?tp=&amp;arnumber=8244278&amp;isnumber=8274985</a></div><div><br /></div><div>A poster</div><div><a href=\"http://www.eccv2016.org/files/posters/P-2A-13.pdf\">http://www.eccv2016.org/files/posters/P-2A-13.pdf</a></div><div><br /></div><div><br /></div><div><br /></div><div><br /></div><div>Thoughts:</div><div>Even humans would have to know what an ‘action’ is, so some labels are needed. But would be really inefficient to label everything. Semi-supervised approaches sound great. Youtube 8M has machine generated annotations may not be for actions.</div><div>Since robots will be imitating human actions, would it be important to look at recent works in pose detection?</div><div><br /></div><div>Datasets</div><div>MPII Cooking dataset (open to download) <a href=\"https://www.mpi-inf.mpg.de/departments/computer-vision-and-multimodal-computing/research/human-activity-recognition/mpii-cooking-activities-dataset/\">https://www.mpi-inf.mpg.de/departments/computer-vision-and-multimodal-computing/research/human-activity-recognition/mpii-cooking-activities-dataset/</a></div><div>ICPR 2012 Kitchen dataset <a href=\"http://www.murase.m.is.nagoya-u.ac.jp/KSCGR/download.html\">http://www.murase.m.is.nagoya-u.ac.jp/KSCGR/download.html</a></div><div><br /></div><div><br /></div><div><br /></div></div>"
    }
  ]
}