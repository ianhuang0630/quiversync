{
  "title": "June 02: Video Frame Embedding project",
  "cells": [
    {
      "type": "text",
      "data": "<div style=\"word-wrap: break-word; -webkit-nbsp-mode: space; -webkit-line-break: after-white-space; white-space: normal;\"><div>Past work:</div><div><br/></div><div>Video Concept Embedding</div><div><a href=\"https://pdfs.semanticscholar.org/a4c4/30b7d849a8f23713dc283794d8c1782198b2.pdf\">https://pdfs.semanticscholar.org/a4c4/30b7d849a8f23713dc283794d8c1782198b2.pdf</a></div><div><ul><li>uses a concept histogram from prior work</li><li><br/></li></ul></div><div><br/></div><div>Learning Temporal Embeddings for Complex Video Analysis</div><div><a href=\"http://vision.stanford.edu/pdf/RamanathanICCV2015.pdf\">http://vision.stanford.edu/pdf/RamanathanICCV2015.pdf</a></div><div><ul><li>Uses simple architecture — adapted from “standard CNN architecture”, learnt an embedding ontop of “fully connected fc6 layer” (<span style=\"font-weight: bold;\">reference available in the paper</span>)</li><li>no audio information is used</li><li>Embeddings are done based on frames, not on a video clip (means that you’re really just embedding images with consideration for the context)</li><li>has pretty low mAP — 26%, means there’s room for improvement</li><li><span style=\"font-weight: bold;\">Simplistic context representation</span>: the way they do their context vectors do not consider the order in which events happen (as of now, the context vector is merely a sum of different vectors within a certain window). window sizes are constants set for every experiment. </li><ul><li>what if you dynamically weighted every single frame, increasing the weight as it gets closer and closer to the target frame?</li></ul><li>What to make of the embedding loss objective? you’re trying to maximize the difference between two embeddings, but …</li><ul><li>A, B relation when A,B are never in the same video</li></ul><li><b>Initializing embeddings with word2vec, and predicting a <i>perturbation</i> to every vector in the embedding space. </b></li><li><b>Graphic models</b></li></ul></div><div><br/></div><div><br/></div><div>CROSS-MODAL EMBEDDINGS FOR VIDEO AND AUDIO RETRIEVAL</div><div><a href=\"https://arxiv.org/pdf/1801.02200.pdf\">https://arxiv.org/pdf/1801.02200.pdf</a></div><div><ul><li>about tying visual and audio together, when they’re  each individually given. </li><li>Not immediately relevant to the project</li></ul><div><br/></div></div><div><br/></div><div><br/></div><div><br/></div><div><br/></div><div><span style=\"font-weight: bold;\">What if we’d like to supplement the word embeddings with video embeddings? And use the word embeddings to further the video embeddings?</span></div><div><span style=\"font-weight: bold;\">Combining VISUAL, AUDIO and TEXTUAL information</span></div><div><br/></div><div><br/></div><div><br/></div></div>"
    }
  ]
}