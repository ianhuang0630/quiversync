{
  "title": "July 18: Action embeddings writeup",
  "cells": [
    {
      "type": "text",
      "data": "<div style=\"word-wrap: break-word; -webkit-nbsp-mode: space; -webkit-line-break: after-white-space; white-space: normal;\"><div>Research question</div><div>How can we create an embedding for <span style=\"font-style: italic;\">sequences</span> of frames to represent the semantic similarity between actions/motions?</div><div><br /></div><div>Within computer vision, work on developing embeddings has mostly been driven by image classification and object detection, and have enabled learning to happen with some pretext of the similarity between the target classes. For video analysis and classification purposes, current approaches still seem to stick with image embeddings for individual frames. The embeddings themselves only capture the composition of a frame, and not the action that’s happening in a sequence of frames, and so, whereas image embedding is an abstraction of concepts for pixels, there is room for an additional layer of abstraction across sequences of frames. I believe developing such embeddings would not only help with video analysis tasks, but allow systems to compare the similarity between two actions in two different video clips.</div><div><br /></div><div>As far as I can tell, the topic of developing embeddings for <i>actions</i> was most closely addressed by FeiFei Li and her group in “<a href=\"http://vision.stanford.edu/pdf/RamanathanICCV2015.pdf\">Learning Temporal Embeddings for Complex Video Analysis</a>”. Other papers that are marginally related include “<a href=\"https://pdfs.semanticscholar.org/a4c4/30b7d849a8f23713dc283794d8c1782198b2.pdf\">Video Concept Embedding</a>” and “<a href=\"https://arxiv.org/pdf/1801.02200.pdf\">Cross-Modal Embeddings for Video and Audio Retrieval</a>”. The embeddings developed by Ramanathan et al. were embeddings of individual frames, since every datapoint within the embedding space is a single image. While the embeddings are trained in these works with consideration to the temporal context, a vector within the embedded space does not hold any temporal information at all. Here’s an example for why this matters: suppose you have a video of a person grabbing the handle on a stationary cup and lifting the cup off of the table (video A) and another where the person is putting the cup down on the table (video B). While the actions are opposites, for every frame within video A, there should be a very similar frame within video B (A is just the reverse of B). Given that embeddings are developed with temporal context, the embeddings for the same frame in A would be put in two completely different zones within the embedding space, even when representing similar concepts within the image (i.e. hand <i>holding</i> cup, not hand <i>moving </i>cup up or down). While embeddings are often used to evaluate similarity between two things, the embeddings developed in these works can only compare the similarity between frames, and not sequence of frames.</div><div><br /></div><div>One of the biggest challenges in this research project would be determining the granularity to which an “action” is defined. This could be done using unsupervised approaches or supervised, as there are labeled video datasets out there with labeled actions. Once this is done, embeddings could be extracted using a multi-frames version of the skip-gram model (i.e. each “action” , or sequence of frames is used to predict the neighboring sequence.). If actions are learned using a supervised approach, Initializing the embedding vectors can be done with known word embeddings that already encapsulate the semantic similarity between concepts labeled for every video clip. Otherwise, if some title is available to each video (e.g. Youtube 8-M) vector initialization can be done based on the word embeddings of the title of the video. Of course, this is a very naive approach, and may not even work, but seen as the skip-gram model is often the go-to approach  when training for embeddings (along with training embeddings along with an auxiliary task — but this demands a suitable task), we may experiment with that first. Developing these embeddings can be enablers for systems to further understand continuous actions, as well as learn to recognize their similarities. </div><div><br /></div><div><br /></div><div><br /></div><div><br /></div></div>"
    }
  ]
}