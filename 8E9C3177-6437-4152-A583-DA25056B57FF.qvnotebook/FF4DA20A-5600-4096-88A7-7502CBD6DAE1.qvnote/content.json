{
  "title": "Nov 4: Suggested Paper by Eugene: Generative Netowrks with Metric Embeddings",
  "cells": [
    {
      "type": "text",
      "data": "<div style=\"word-wrap: break-word; -webkit-nbsp-mode: space; -webkit-line-break: after-white-space; white-space: normal;\"><div><a href=\"https://arxiv.org/pdf/1805.07674.pdf\">https://arxiv.org/pdf/1805.07674.pdf</a></div><div><b>Code: </b><b><a href=\"https://github.com/a554b554/BourGAN\">https://github.com/a554b554/BourGAN</a></b></div><ul><li><div>Main contributions: </div></li><ul><li><div>When there are “separated modes”, drawing random vectors from a single Gaussian gives “large gradients of the generator”, which is inferior to drawing from a mixture model. </div></li><li><div>Interpretability of these modes increase under <span style=\"font-weight: bold;\">a specific distance metric.</span></div></li></ul><li><div>Based on metric embedding theory, bourgain theorem</div></li><li><div>Draws random vectors from a Gaussian mixture model on a low-dimension latent space</div></li></ul><div><br /></div><div>The output distribution matches the input distribution (in the latent space) through adjusting the cost function. (See paper for details)</div><div>Maybe similar thing can be done to match the embeddings of a classification model (final layers) to that of the Bourgain algorithm output of embeddings. So </div><div><img src='quiver-image-url/59F0B691573D87F6F53474CA0ED5017C.jpg' width='580'><br /></div><div><br /></div><div>Once new class is given, the final few layers of the model may have to change, in which case the final layers can be separately trained faster. (Or is there a smarter way to do that?)</div><div><br /></div><div>We could test this on the shape dataset first. <span style=\"font-weight: bold;\">How do embeddings hold properties of individuals objects? Would there be miniature clusters within bigger clusters in the embedding spaces, for colors, for example? And if so, how can we exploit this? If not, how can we construct this?</span></div><div><br /></div><div>This idea seems to show a lot of promise. Next steps:</div><div><input type=\"checkbox\" checked=\"false\" />pass idea through carl</div><div><input type=\"checkbox\" checked=\"false\" />give this a try on the MNIST dataset. Learn some key fundamental digits (that cosmetically look different from eachother (e.g. 0, 1)) and then see how well it does if you tell it to recognize a new number, given how different it is from other numbers (information provided by human trainer)</div><div><br /></div><div><br /></div><div><br /></div><div>More on mode collapse:</div><div><input type=\"checkbox\" checked=\"false\" /><a href=\"http://aiden.nibali.org/blog/2017-01-18-mode-collapse-gans/\">http://aiden.nibali.org/blog/2017-01-18-mode-collapse-gans/</a></div></div>"
    }
  ]
}