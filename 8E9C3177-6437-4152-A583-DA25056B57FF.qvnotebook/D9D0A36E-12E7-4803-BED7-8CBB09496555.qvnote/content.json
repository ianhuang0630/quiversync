{
  "title": "Sept 5: Thoughts on Prof Allen's Proposal",
  "cells": [
    {
      "type": "text",
      "data": "<div style=\"word-wrap: break-word; -webkit-nbsp-mode: space; -webkit-line-break: after-white-space; white-space: normal;\"><div><span> Tasks of interest: Navigation + manipulation</span><br /></div><div><br /></div><div>We want to specifically exploit 3D depth data readily available from the mobile robot’s range sensor in</div><div>real-time as both a training/learning input and a navigation input to allow the robot to move seamlessly</div><div>in novel, cluttered environments using a CNN to train for 3D obstacles.</div><ul><li><div>what’s the difference between training and navigation input?</div></li></ul><div><br /></div><div>3D depth data -&gt; dense feature vector?</div><div><br /></div><div>does the agent have general directions of the room? Is it navigating and searching at the same time, or does it know the general direction to go? How is this different from what BD is doing?</div><div><br /></div><div><br /></div><div>Embeddings could be used to record the actions of walking through a hallway, or pushing a door open. These are actions that make up the whole journey. Embed these, and actions can be more modular? It will learn to navigate through hallways (generally) as it’s navigating to the target. This may allow it to be generalizable in different conditions. Embeddings could be developed on vectorized values of sequence of frames, as well as context actions.</div><div><br /></div><div><br /></div><div>Deep TAMER</div><div><a href=\"https://arxiv.org/pdf/1709.10163.pdf\">https://arxiv.org/pdf/1709.10163.pdf</a></div><div><br /></div><div>The embeddings under Deep TAMER are simply encoder/decoder embeddings, for dimensionality reduction. they themselves don’t capture any semantic relationship between different scenes. What benefits would arise if they had relationships (context similarity, time closeness…etc)</div><div>This may have worked fine for Atari bowling, but what navigation through rooms is an even higher dimensional space, so some structure in the image embeddings may help.<b> Also work doesn’t include depth information. </b>So we could get new embeddings.</div><div><br /></div><div>Embeddings similar if similar route of navigation? This would increase generalizability. RGB-D —&gt; embedding, finer adjustment ontop of this… We’d have to design a way to measure route similarity.</div><div></div><div>Also, deep TAMER doesn’t do experiments where the agent has to traverse different parts of the room. Would you punish a robot for going the wrong direction (humans would make the same mistake), or would you simply punish it for bumping into things? </div><div><br /></div><div><br /></div><div><br /></div><div><b>New idea;</b></div><div>For the deep TAMER part, the computer will create embeddings that group physical scenarios together if they can be similarly traversed. This has the aim of speeding up learning, and making things more generalizable</div><div><br /></div><div><br /></div><div><br /></div><div><br /></div><div><br /></div><div><br /></div><div><br /></div></div>"
    }
  ]
}