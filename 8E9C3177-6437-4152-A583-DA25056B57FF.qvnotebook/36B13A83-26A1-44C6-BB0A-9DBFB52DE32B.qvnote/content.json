{
  "title": "July 20: Action Embedding application to robotics: Better than Time Contrasted Networks",
  "cells": [
    {
      "type": "text",
      "data": "<div style=\"word-wrap: break-word; -webkit-nbsp-mode: space; -webkit-line-break: after-white-space; white-space: normal;\"><div><br /></div><div><span style=\"font-weight: bold;\">Time contrastive networks</span></div><div><a href=\"https://arxiv.org/pdf/1704.06888.pdf\">https://arxiv.org/pdf/1704.06888.pdf</a></div><div><br /></div><ul><li><div>how does one know that similar actions with the same function in different time periods  of the video are not pulled apart in the embedding space? (By the same amount as something completely opposite functionally?)</div></li><li><div>Is still looking at embeddings of images, not sets of images.</div></li></ul><div> </div><div><br /></div><div><br /></div><div>A similarity is only assessed based on whether it occurs at the same time, and not whether it’s part of the same action. This essentially trains an arm that learns to imitate  an action ‘frame by frame’ by first learning to evaluate some measure of similarity between two frames (functionality similarity, not content similarity).  How can we say that the robot system has actually learned what a generic ‘pouring the water’ looks like? To go beyond ‘imitation’?</div><div><br /></div><div>When a human has to ‘copy’ another person pouring water, they don’t match things frame by frame, but rather recall how to do it themselves. This means that they have to gather information and generalize what the action of ‘pouring water’ looks like. And then, from a higher level, judge whether what they’re doing with their hands resembles that of pouring water.</div><div><br /></div><div>Action embeddings allow us to go from observing the frames within a video to finding the closest actions that correspond to the observed action.</div><div><br /></div><div>If we had a function that mapped a sequence of frames to an embedding, we could create a reward function that, instead of matching the embeddings between the video demonstration sequence and embeddings of the robot task execution <span style=\"font-style: italic;\">frame by frame, </span>matches by the action embeddings. Matching frame by frame may be problematic, since the TCN embeddings are not invariant between humans and robots during the grasping experiments in the paper. Perhaps reinforcement learning can be used to optimize the reward function, as the paper has currently done.</div><div><br /></div><div>Developing action embeddings would also allow the robot to learn multiple actions within the same video (i.e. if the video is composed of opening a bottle, pouring the liquid into the cup, closing the bottle, and raising the cup). This allows the robot to learn actions for a much more widely accessible set of videos, since no splitting would be necessary.</div><div><br /></div><div>Ideal approach could be combining different angles, as well as action embeddings, (every point in the embedding space is a tuple of sequences of frames), so that we have temporal neighbors.</div><div><br /></div><div><br /></div><div>\"An interesting avenue for future work would be to study how multiple tasks could be embedded in the same space, essentially creating a universal representation for imitation of object interaction behaviors. While in this paper we explored learning representations using time as a supervision signal, in the future, models should learn simultaneously from a collection of diverse yet complementary signals.\"</div><div><br /></div><div><br /></div><div><span style=\"font-weight: bold;\">1 paragraph summary</span></div><div><br /></div><div>Given that we can develop an embedding for actions, which we will define as a sequence of frames (a 4 dimensional block of data), we can compare the similarity between any two actions. This would not only allow robots to better recognize actions, but may also have potential in improving a robot’s ability to imitate human actions. In <i>Time Contrasted Networks</i>, Sermanet et al. developed frame-level embeddings that allow a robot to compare the embeddings between the video demonstration sequence and video footage of the robot task execution, frame by frame. This similarity was used as a reward function for reinforcement learning to learn a robotic arm motion that maximized frame-by-frame similarity to a video with a human demonstrating an action. However, the embeddings developed by this group is only used to compare the functional similarity of different frames taken of the same thing at different angles, rather than similarity between actions. As a result, the robot learns only how to imitate, and not does not develop a better and more general understanding of the composition of the action its imitating through multiple imitations. Furthermore, matching frame by frame may disregard the fact that the TCN embeddings are not invariant between human and robots, since the training data features human demonstrations exclusively. Both of these may be countered by action embeddings, since action embeddings would both allow the robot to compare the similarity between actions and also learn to recognize the similarity between <i>motion</i> of features within frames, rather than focus exclusively on the composition of the frame. As such, this higher level of abstraction may help to develop embeddings that are slightly more invariant to human hand and robot hand differences. To my rather limited knowledge of reinforcement learning, we could define the reward function by some similarity measure between the action embeddings of the video demonstration and those of the robotic arm movement, and use reinforcement learning to stochastically optimize the robot’s motion to match the demonstration. This would allow a much deeper understanding of actions through imitation, as robots would be able to represent how one demonstration fits with the others that it has seen. Perhaps, when the embeddings are well developed, the robot could find the k-nearest neighbors to a demonstration’s action embedding, and regurgitate an action that matches some combination of those embeddings — a method that would no longer be imitation, but rather recognition and recall of actions it has seen.</div></div>"
    }
  ]
}