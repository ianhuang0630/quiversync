{
  "title": "Nov 12: Interpretation of deep neural networks",
  "cells": [
    {
      "type": "text",
      "data": "<div style=\"word-wrap: break-word; -webkit-nbsp-mode: space; -webkit-line-break: after-white-space; white-space: normal;\"><div>Wjociech samek</div><div>Relevance in heatmaps. Sensitivity analysis. You tweek a pixel and see how the function changes. But this is old and naive.</div><ol><li><div>Doesn't explain the pred but the variation of the function. </div></li><li><div>Gradient shattering, explanations can be very noisy and unreliable</div></li></ol><div>Layerwise relevance propagation</div><ol><li><div>Relevance value to every pixel</div></li><li><div>Summation of relevance is set to the prediction </div></li><li><div>Classification forward prop. Initialixe relevance value to be the output prediction at the end and then back prop through the network, at each layer weighted by he activation</div></li></ol><div><br /></div><div><br /></div><div>Some potential Axioms for good/bad relevances0</div><ol><li><div>Conservation. The relevance should add up to the final prediction. </div></li><li><div>Positivity. Relevance is either zero or nonzero </div></li><li><div>Continuity. Two inputs the same and predictions almost the same , you expect the reasoning should also be the same. Interesting definition of continuity...</div></li><li><div>Selectivity. Model must agree with explanation. Removing relevant features should make the prediction change </div></li></ol><div><br /></div><div>Statistical commoness doesn't mean that it is semantically significant. You shouldn't select model based on just performance. </div><div><br /></div><div>If a deep neural network is pretrained, then less likely to pick up statistically common but irrelevant features. Pretrained gets rid of a lot of bias in the dataset. </div><div><br /></div><div>Context vs features in the bounding box. Generally depending more in features in the bounding box gives better performance to classification of a class. </div><div><br /></div><div><span style=\"font-weight: bold;\">From conversation after talk:</span></div><div><br /></div><div><i>Transferring information between neural networks </i></div><div><i>IEEE ICASSP</i></div><div><i>2018</i></div><div><br /></div><div>If we coerced the attention, then will the accuracy of a retrained DNN be reduced in accuracy? And if not, does that mean it can learn faster oness data given human eye tracking data?</div><div>He doesn't know. </div><div>Encourages me to explore this area. Sight by eye tracking imitation.  </div><div><img src='quiver-image-url/DCBDA49EE532DF2E14A85D4F4D71AA15.jpg' width='3024' title='Attachment'><br /></div><div><en-m<a href='quiver-file-url/'></a>></div><div><br /></div></div>"
    }
  ]
}