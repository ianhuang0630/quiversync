{
  "title": "Jan 25: {N} RNN to FSM for Eugene",
  "cells": [
    {
      "type": "markdown",
      "data": "From Jacobsson's survey \"Rule Extraction from Recurrent Neural Networks: A Taxonomy and Review\" and Wang et. al's work presented in \"A Comparison of Rule Extraction for Different Recurrent Neural Network Models and Grammatical Complexity\" and \"An Empirical Evaluation of Rule Extraction from Recurrent Neural Networks\", I learned that the standard FSM extraction strategy involves first collecting the hidden activations of RNN when processing every string at each time step and then discretizing these into different \"states\", then discovering the transitions from cluster to cluster after every input character, and then finally doing a reduction step to reduce the diagram to a minimal representation of state transition. Usually, the discretization is done by K-means. Since these steps are done in a sequential manner, the discretization is done regardless of the potential transitions that might occur after it. I thought that this was a weakness in the method of generating FSM's, because the quality of the transitions extracted depends heavily on the activation space discretization, and yet the latter is done without consideration for the former. Moreover, because multiple authors (in \"An Empirical Evaluation of Rule Extraction from Recurrent Neural Networks\" and in \"A Comparison of Rule Extraction for Different Recurrent Neural Network Models and Grammatical Complexity\") use K-means clustering and other Euclidean-distance based approaches, the distances between the (hidden) states of an RNN are essentially taken at face-value (K-means makes the implicit assumption that a cluster's shape is spherical, which may not be the case). \n\nThis was reaffirmed when I ran my implementations of the FSM extraction algorithms, and discovered that the transitions between discretized states (using K-means) had transitions with very high entropy. As a result, what we initially intended to be a FSM looked much more like a Markov model.\n\nHowever, there may be a representation of the same activations that does not lie in the Euclidean space that better clusters the hidden states of RNN's such that lower-entropy transitions are yielded later on when FSM transitions are extracted. I hypothesized that perhaps this could be realized by doing some sort of metric learning (perhaps a Mahalanobis distance) on the hidden activation space. To do such metric learning, I thought that one could use the similarity of the input (perhaps with some sort of decay with time) as the ground-truth similarity, or some other measure that represents the expected level of entropy in the transitions once the transition-extraction step is completed. Unfortunately, I never got to experiment with this idea.\n\nJohn F. Kolen states in \"Fool's Gold: Extracting Finite State Machines from Recurrent Network Dynamics\" that because of the sensitivity to initial conditions, one can't derive useful FSM representations from an RNN. His analysis, however, was done simply from the Euclidean activation space. Perhaps, after applying the relevant transformation on the raw activation space (derived from the learned metric), the system would be less prone to instability due to initial activations.\n\nAnother weakness that exists in the typical extraction method was the fact that one has to select the number of clusters (or the number of states) to discretize the activation space into. But without having a FSM in mind prior to extracting the rules, it's very hard to select the number of clusters in the first place. Empirically, this was done in Wang et al.'s paper by running the extractor on many different values of K in some predefined range, and selecting the one that yields the best silhouette coefficient. I thought it would be interesting to optimize K for minimal entropy in transitions (after those are extracted following the discretization) instead. This way, in some sense, the choice of clustering takes into account some information from the transition extraction task that follows directly after, and is not solely based on the activation space.\n\nOn a more philosophical side however, I had doubts about the interpretability of the FSM's as the RNN's they represented became more and more complex. The whole purpose for extracting FSM's was to be able to interpret something that isn't as well-interpretable (hidden states of RNN's), but as the RNN becomes bigger and more complex, we'd expect the number of states in the extracted FSM to increase as well. Moreover, the meaning of the states (what each of them represented semantically) would not come with the extraction. As such, if the extractor arrives at an FSM that has 89 states, I was not sure how much more interpretable that would be compared to the raw hidden activation values.\n"
    },
    {
      "type": "markdown",
      "data": "\n"
    }
  ]
}