{
  "title": "Mar 21: {N} Summer Research Proposal",
  "cells": [
    {
      "type": "markdown",
      "data": "Much of computer vision and video analysis has focused on the correct identification of entities and activities. As much as identification is important, it is also essential that an autonomous agent knows how to manipulate an object. While objects are oftentimes rigid bodies in real life, their mechanical structure may allow for multiple configurations. (E.g. a car's wheels are free to turn along the axle of rotation, but also turn at an angle to the car body.) In this project, we hope to be able to teach a machine learning system how to identify varying degrees of freedom (as well as the categories of different motions) in the movement within a single entity. An example would be showing how a ball point pen allows for one end (the plunger) to \"sink\" into the pen (movement along one axis). Another example would be the rotational motion that a 3x3 rubics cube allows along multiple axes.\n\nWe can start with using synthetic data to generate mechanisms in Blender (or some other graphical design software) that specifically demonstrate certain types of motions (e.g. a door hinge, for rotational motion), and then create a video dataset of their movement in the simulated environment. After training on this dataset of many different types of motions, we would expect that the computer be able to pinpoint the motion type (e.g. rotational joint) and relevant axes (e.g. in the previous example, the axis of rotation). From here, we may then proceed onto learning from real world data.\n\nSo far, I haven't found any relevant papers that have tackled this issue. This however is a very important issue to investigate, since able to learn to manipulate objects from video demonstration requires some learning of degrees of freedom within that object. Thus, the ability to build up this mental model of ways in which configurations of objects from visual demonstration seems like a problem worth solving."
    }
  ]
}