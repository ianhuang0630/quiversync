{
  "title": "Dec 31: {N/ P} Suggested Online Learning Papers",
  "cells": [
    {
      "type": "markdown",
      "data": "## Parent note\n[Dec 26: investigating infinite dimensional latent spaces -- what's been done already?](quiver-note-url/D75B8191-D52C-4345-AD1B-FE5AC17D6D7C)"
    },
    {
      "type": "markdown",
      "data": "## Resources\nSome papers to read:\nOnline metric learning:\n- [POLA paper](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.442.5929&rep=rep1&type=pdf) by Shai Shalev Schwartz and friends.\n- [LEGO paper](http://people.bu.edu/bkulis/pubs/nips_online.pdf) by Jain and friends\n- [RDML](https://www.cse.msu.edu/~rongjin/publications/nips10-dist-learn.pdf) by Jin et al.\n\nPotentially relevant:\n- [Deep Online metric learning](https://arxiv.org/pdf/1805.05510.pdf)"
    },
    {
      "type": "markdown",
      "data": "## Overall thoughts\nPola's nice, but doesn't seem to do provide any guarantees on catastrophic forgetting of online metric learning (not even compare it to accuracy of batch setting)."
    },
    {
      "type": "markdown",
      "data": "## Pola"
    },
    {
      "type": "markdown",
      "data": "### Summary"
    },
    {
      "type": "latex",
      "language": "latex",
      "data": "Online algorithm for metric learning. Basically splits the algorithm into two parts. \n\nFirst part is a projection onto the set of matrices $A$ that satisfy the equation\n$y_\\tau (b_\\tau - v_\\tau^T A v_\\tau) \\geq 1$, where $v_\\tau^T = x_\\tau - x_\\tau'$ (difference between two data vectors), and  $y_\\tau = 1$ if two samples $x_\\tau$ and $x_\\tau'$ are similar, or $-1$ otherwise, and $b_\\tau$ is a threshold. They represent $A$ and $b_\\tau$ as a single vector in $\\mathbb R ^{n^2 + 1}$, where $A$ is listed columnwise and $b_\\tau$ is tacked onto the end of the vector. They represent $(-y_\\tau v_\\tau v_\\tau^T, y_\\tau)$ sa $\\mathcal X_\\tau \\in \\mathbb R^{n^2 +1}$. They thus reduce the constraint that $y_\\tau (b_\\tau - v_\\tau^T A v_\\tau) \\geq 1$ to $w_\\tau \\cdot \\mathcal X_\\tau \\geq 1$. This projection is given by \n$$ \\mathcal P_{C_\\tau}(w) = w_\\tau + \\alpha \\mathcal X_{\\tau}$$\nwhere $\\alpha = (1- w_\\tau \\cdot \\mathcal X_\\tau)/ || \\mathcal X_\\tau||^2$. (I verified this result using the Lagrange equation for equally constraints.)\n\nThe second part is a projection onto the set of all positive semidefinite matrices, which is given by decomposing the original matrix down to eigenvalues and eigenvectors, and then deleting the eigenvectors and values that corresponds to any negative eigenvalues. The \"Eigenvalue interlacing theorem\" tells us that $A$ has at most one single negative eignevalue.\n\nThey also show that the algorithm is kernelizable. They showed that one can express any $A$ as $A = \\sum_{i=1}^m \\beta_i r_i r_i^T$ where $m \\leq 2\\tau$. (why $2\\tau$?) \n\nWhen not assuming that there exists a match, we have a relaxation parameter $\\gamma$. The algorithm's update parameter $\\alpha$ would be\n$$ \\alpha = \\frac{\\mathcal l_\\tau (A_\\tau, b_\\tau)}{|| x_\\tau- x'_\\tau||^4 + 1 + \\gamma} $$\nI'm guessing this can also be derived from constrained optimization for a given $\\gamma$. They say that \"Since there is no perfect pseudo-metric that explains the data ..., we do not expect our online algorithm to attain a fixed amount of loss. Instead we measure the loss of the outline algorithm relative to the loss of any other fixed pseudo metric parametrized by $(A^*, b^*)$.\"\n"
    },
    {
      "type": "markdown",
      "data": "### Theoretical guarantees"
    },
    {
      "type": "latex",
      "language": "latex",
      "data": "They prove that with $(x_1, x_1', y_1), ..., (x_\\tau, x_\\tau', y_\\tau)$ be a sequence of examples and $R$ be an upper bound such that $\\forall \\tau: R \\geq ||x_\\tau- x_\\tau'||^4 + 1$. If there exists $A^*$ that is positive definite adn $b^* \\geq 1$ for which $\\forall \\tau \\geq 1, \\mathcal l_\\tau(A^*, b^*) = 0$, then for any $T \\geq 1$:\n  $$\\sum_{\\tau=1}^T(\\mathcal l_\\tau(A_\\tau, b_\\tau))^2 \\leq R (||A^*||^2 + (b^*-b_1)^2)$$\nWhere $\\mathcal l_\\tau$ is the hinge loss $\\max(0, y_\\tau((d_A(x_\\tau, x_\\tau'))^2 - b) + 1)$. \nThe above used a lemma proved by Censor & Zenios, 1997, Thm 2.4.1. (It's a book...) Let $w^* \\in \\mathbb R^n$ and let $C \\in \\mathbb R^n$ be a closed convex set. Then for any $w^* \\in C$, we have\n  $$|| w - w^*||^2 - || \\mathcal P_C(w) - w^* ||^2 \\geq ||w - \\mathcal P_C(w)||^2$$\nNote that the bound is on the sum of *squares* of the hinge losses.\n\nNote that the bound doesn't depend on the dimensions of the input space. The bound doesn't change if we employ kernels whcih map the instances to higher dimensional maps. (How's this different from learning the $\\sqrt A = T$? Is this related to analysis that I did in:"
    },
    {
      "type": "markdown",
      "data": "[Jan 6: Deeper thought into incrementing dimensions of L](quiver-note-url/8ECFB05F-F6C8-4286-9C1B-D9B2EE8172F4) ?)"
    },
    {
      "type": "markdown",
      "data": "Also this guarantee says nothing about _catastrophic forgetting_, as it's learning online..."
    },
    {
      "type": "markdown",
      "data": "### Results:\nKNN with learned metric shows better performance in binary classification than raw-KNN on the default dataspace. Other experiments prove that POLA's effective."
    },
    {
      "type": "markdown",
      "data": "## LEGO\n### Summary"
    },
    {
      "type": "latex",
      "language": "latex",
      "data": "Built upon LogDet regularization, and uses gradient descent. Hard part in the real world when actually using metric learning is fetching the closest neighbors. Current approaches (trees etc.) only deal with static metrics. Once it changes, it becomes very expensive to recompute the tree to fetch the nearest neighbors.\n\nUnlike POLA, doesn't have to compute eignevectors, so \"considerably faster\". Faster search is made possible by \"locality-sensitive hashkeys\" during online learning.\n\nThey focus on a variant of metric learning where a quadruple is provided $(u_t, v_t, y_t, b_t)$ where $u_t$ and $v_t$ are two samples and $y_t$ is a distance, and $b_t \\in \\{-1, 1\\}$ indicates whether the two samples should be *at least* or *at most* $y_t$ away from eachother.\n\nUpdate rule:\n$$ A_{t+1} = argmin_{A \\succ 0} D(A, A_t) + \\eta \\mathcal l(d_A(u_t, v_t), y_t)$$\nUsing LogDet divergence as a regularization function, \n$$D_{ld}(A, A_t) = tr(A A_t^{-1}) - \\log \\det(AA_t^{-1}) - d$$\n(What is $d$ ?) The loss $\\mathcal l$ is goign to be taken as the squared loss $\\frac{1}{2} (\\hat y_t - y_t)^2$, with $\\hat y_t = d_A(u_t, v_t)$. \n\nUisng \"straightforward algebra and Sherman-Morrison inverse formula\", $A_{t+1}$ can be analyticxally calculated as:\n$$ A_{t+1} = A_t - \\frac{\\eta(\\bar{y} - y_t) A_t z_t z_t^T A_t}{1 + \\eta(\\bar{y} - y_t) z_t^T A_t z_t}$$\nwhere $z_t = u_t - v_t$ and $\\bar{y} = d_{A_{t+1}} (u_t, v_t) = z_t^T A_{t+1}z_t$. But since $\\bar{y}$ is dependent on $A_{t+1}$, we calculate this by deriving it from multiplying $z_t^TA_{t+1}z_t$, and getting (refer to paper):\n$$ \\bar y = \\frac{\\eta y_t \\hat{y}_t - 1 + \\sqrt{(\\eta y_t \\hat{y_t} - 1) ^2 + 4 \\eta \\hat{y}_t^2}}{2\\eta \\hat{y}_t}$$\n\n---------------------------------------\nDetour: Sherman-Morrison inverse formula:\nInvertible $A$, vectors $u$ and $v$. $A+uv^T$ invertible iff $1+v^TA^{-1}u \\not = 0$. Inverse given by:\n$$(A + uv^T)^{-1} = A^{-1} - \\frac{A^{-1} u v^T A^{-1}}{1 + v^TA^{-1}u}$$\n============================\n\nThe resulatant $A_{t+1}$ is guaranteed to be positive definite if $A_t$ is. This means no projection methods (like POLA) is needed to project it onto the PD cone. Since maintains PD, then the regularization parameter needs not be changed according to the current contraint (unlike information-theoretical approaches like ITML).\n"
    },
    {
      "type": "markdown",
      "data": "### Theoretical guarantees"
    },
    {
      "type": "latex",
      "language": "latex",
      "data": "$L_A$ is the aggregate loss through timesteps as $A$ changes\n$L_{A^*}$ is the minimal loss in an offline setting.\nAssuming input vector lengths are bounded $|| \\vec{u} ||^2 \\leq R$\n\n$L_A \\leq c_1 L_{A^*} + c_2$\nMore specifically,\n$$ c_1 = 1 + \\eta \n\\biggr (\n\\frac{R}{2} + \\sqrt{\\frac{R^2}{4} + \\frac{1}{\\eta}} \\biggr)^2$$\n$$ c_2 = \n\\frac{1}{\\eta} + \\biggr ( \n\\frac{R}{2} + \\biggr ( \n\\frac{R}{2} + \\sqrt{\\frac{R^2}{4} + \\frac{1}{\\eta}} \\biggr ) ^2 \n\\biggr )$$\n\nThere are formal proofs but they are in the longer form of the paper."
    },
    {
      "type": "markdown",
      "data": "## RDML\n### Summary\n- They examine generalization of regularized distance metric learning, and found that (with appropriate constraints) the generalization error could be independent form the dimensionality (Of the input data ?).\n- They introduce an efficient online metric learning algorithm"
    },
    {
      "type": "markdown",
      "data": "## Deep Online metric learning\n### Summary"
    },
    {
      "type": "markdown",
      "data": ""
    }
  ]
}